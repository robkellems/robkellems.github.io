<!DOCTYPE html>
<html>
<head>
    <title>Robert Kellems</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>
    <style>
        body {
            background-color: #1b1c1d;
            color: white;
        }
        .aligned.menu {
            display: flex;
            justify-content: flex-start;
            flex-wrap: wrap;
        }
        .content-container {
            margin-left: 200px;
            margin-right: 200px;
        }
        .header-menu-container {
            display: flex;
            flex-direction: column;
        }
        .header-menu-container > .ui.header.inverted {
            margin-top: 0;
        }
        .image {
            object-fit: cover;
            display: block;
            margin-left: auto;
            margin-right: auto;
        }
        #image-stream {
            width: 370px;
            height: 350px;
        }
        #image-aifuture {
            width: 400px;
            height: 330px;
        }
        #image-reviews {
            width: 440px;
            height: 200px;
        }
        #image-hackathon {
            width:425px;
            height:350px;
        }
        #image-spectro {
            width: 450px;
            height: 350px;
        }
        #image-pca {
            width: 400px;
            height: 400px;
        }
        #image-asa {
            width: 425px;
            height: 350px;
        }
        .content {
            display: none;
        }
        .show-content {
            display: block !important;
        }
        p {
            font-size: 17px;
        }
        h3 {
            cursor: pointer;
            transition: color 0.3s ease;
        }
        h6 {
            text-align: center;
        }
        h3:hover {
            color: #ffcc00; /* Update to the desired color */
        }
    </style>
    <script>
        function toggleContent(id) {
            var content = document.getElementById(id);
            content.classList.toggle("show-content");

            var listItem = document.querySelector(`#${id}`).closest('li');
            listItem.scrollIntoView({ behavior: 'smooth', block: 'start' });
        }
    </script>
</head>
<body>
<br>
<div class="ui container">
    <div class="content-container">
        <div class="header-menu-container">
            <h1 class="ui header inverted">Robert Kellems</h1>
            <div class="ui inverted aligned menu">
                <a href="index.html" class="item">About</a>
                <a href="projects.html" class="item">Projects</a>
                <a href="resume.html" class="item">Resume</a>
            </div>
        </div>
        <br>
        <h4>Click each project to learn more.</h4>
        <h2><u>Current Project(s):</u></h2>
        <ul>
            <li>
                <h3 onclick="toggleContent('content1')"><b>Auditory Streaming Research</b></h3>
                <div id="content1" class="content">
                    <br>
                    <img class="ui image" id="image-stream" src="resources/streaming.gif" alt="Animation demonstrating the UI for this project">
                    <br>
                    <p><a href="https://en.wikipedia.org/wiki/Auditory_scene_analysis" target="_blank">Auditory streaming</a> is a commonly used model for thinking about how our brains organize all of the sound hitting our eardrums at any given time. The main idea is that we preattentively separate sound into individual, coherent “auditory streams” using features such as pitch, temporality, and timbre. I am interested in investigating possible high-level cues that affect how we create perceived note groupings (formed via the process of auditory streaming) when listening to short melodic patterns.</p>
                    <p>To gather data on this phenomenon, I've created a novel interface which allows subjects to listen to short melodic loops and map their perceived note groupings onto a grid sequencer by drawing lines. Once one completes the experiment, I can gather their drawn connections and recreate them on my own machine to label them based on key aspects. With these data, I have been reaching conclusions about high-level principles which play a part in our auditory perception.</p>
                    <p><a href="https://pcl.sitehost.iu.edu/robsexperiments/tests&examples/auditoryStreaming/experiment/" target="_blank">Try the experiment for yourself here!</a> (click "Continue" at the bottom of the first page to view the experiment)</p>
                    <p><b>Awards:</b></p>
                    <ul>
                        <li><p><a href="https://cogs.indiana.edu/student-portal/undergraduate/murray-austin-goldstone-scholarship.html">Murray Austin Goldstone Scholarship for Undergraduate Research</a></p></li>
                        <li><p><a href="https://cogsci.northwestern.edu/undergraduate/undergraduate-award.html" target="_blank">Robert J. Glushko Research Excellence Award for Outstanding Oral Presentation</a></p></li>
                    </ul>
                </div>
            </li>
        </ul>
        

        <h2><u>Completed Projects:</u></h2>
        <h2>2023:</h2>
        <ul>
            <li>
                <h3 onclick="toggleContent('content2')"><b>AI Futures Artificial Intelligence Ground Vehicle Challenge @ Purdue (1st Place)</b></h3>
                <div id="content2" class="content">
                    <br>
                    <img class="ui image" id="image-aifuture" src="resources/aiFutures.jpg" alt="Picture from the AI Futures competition">
                    <h6><i><a href="https://www.linkedin.com/posts/aifutures_purdueuniversity-autonomousvehicles-artificialintelligence-activity-7025530933269209089-NPUf/?utm_source=share&utm_medium=member_desktop" target="_blank">Image source</a></i></h5>
                    <p>I participated in a capture the flag style competition with a team of fellow undergraduates in which we remotely connected to a robot using Linux and ROS Noetic, and then implemented algorithms that allowed the robot to operate autonomously. We utilized Python and OpenCV along with the robot's built-in camera to accomplish basic computer vision tasks, such as real-time classification of left versus right arrows. At the end of the challenge, we had the top score in the undergraduate category.</p>
                </div>
            </li>
        </ul>
        <h2>2022:</h2>
        <ul>
            <li>
                <h3 onclick="toggleContent('content3')"><b>Movie Review Classification w/ Logistic Regression</b></h3>
                <div id="content3" class="content">
                    <img class="ui image" id="image-reviews" src="resources/reviews.png" alt="Examples of data from this project">
                    <br>
                    <p>Using Python's “os” library, I iterated through 50000 movie reviews, each labeled as either positive or negative, and created CSV files containing each review along with its label. After cleaning the text for each review, I utilized Scikit-learn to transform each review into a feature vector which was then fed into a logistic regression model. The average test accuracy of this model was around 89%.</p>
                    <p><a href="resources/LogisticRegressionMovieReviews.pdf" target="_blank">Code</a></p>
                    <br>
                </div>
            </li>
            <li>
                <h3 onclick="toggleContent('content4')"><b>Reminder App</b></h3>
                <div id="content4" class="content">
                    <img class="ui image" id="image-reminder" src="resources/reminder.gif" alt="Short clip of how the app functions">
                    <br>
                    <p>I built an Android application which allows the user to write a note and choose a date/time to be reminded with said note. Each reminder is stored in a Firebase database rather than locally.</p>
                    <p><a href="https://github.com/robkellems/ReminderApp" target="_blank">Github</a></p>
                </div>
            </li>
        </ul>
        <h2>2021:</h2>
        <ul>
            <li>
                <h3 onclick="toggleContent('content5')"><b>JPMorgan Chase Data for Good Hackathon</b></h3>
                <div id="content5" class="content">
                    <img class="ui image" id="image-hackathon" src="resources/hackathon.png" alt="Figure from the presentation">
                    <br>
                    <p>I collaborated with a remote team of undergraduates from different regions across the United States. Our objective was to devise an investment strategy for a New York school board, using a combination of data collected from local schools and online sources. After completing our analysis, we effectively presented our recommendations to the school board members, demonstrating our thorough understanding of the data and our capacity to transform it into actionable insights.</p>
                    <p><a href="resources/JPMCHackathonGroup8.pdf" target="_blank">Presentation Slides</a></p>
                    <p><a href="https://careers.jpmorgan.com/US/en/students/programs/code-for-good?search=&tags=location__Americas__UnitedStatesofAmerica" target="_blank">JPMorgan Chase Social Good Hackathons</a></p>
                    <br>
                </div>
            </li>
            <li>
                <h3 onclick="toggleContent('content6')"><b>Environmental Sound Classification w/ Neural Networks</b></h3>
                <div id="content6" class="content">
                    <img class="ui image" id="image-spectro" src="resources/spectro.png" alt="Spectrogram">
                    <br>
                    <p>I collaborated with a team of fellow students to implement several deep neural network designs in order to classify environmental sound clips using Python and TensorFlow/Keras. We used <a href="https://www.kaggle.com/datasets/chrisfilo/urbansound8k" target="_blank">UrbanSound8K</a>, a dataset containing 8732 labeled sound excerpts, each belonging to one of ten classes (e.g. dog bark, ambulance siren).</p>
                    <p><a href="resources/B351FinalProject.py" target="_blank">Code</a></p>
                    <p><a href="resources/AIFinalResearchPaper.pdf" target="_blank">Paper</a></p>
                    <br>
                </div>
            </li>
            <li>
                <h3 onclick="toggleContent('content7')"><b>Predicting eBay Auction Prices w/ Regression</b></h3>
                <div id="content7" class="content">
                    <img class="ui image" id="image-pca" src="resources/pca.png" alt="PCA from the paper for this project">
                    <br>
                    <p>I worked with a team of fellow undergraduates in order to find potential variables which could assist in predicting the final selling price of smartphones being sold on eBay. Using Python and Beautiful Soup, we scraped data from several auctions and then used R to apply various statistical models to these data.</p>
                    <p><a href="https://github.com/James-Dumas/B365-final-project" target="_blank">Github</a></p>
                    <p><a href="resources/B365FinalReport.pdf" target="_blank">Paper</a></p>
                    <br>
                </div>
            </li>
            <li>
                <h3 onclick="toggleContent('content8')"><b>Auditory Scene Analysis & Musical Experience</b></h3>
                <div id="content8" class="content">
                    <img class="ui image" id="image-asa" src="resources/asa.jpg" alt="Image illustrating auditory scene analysis">
                    <h6><i><a href="https://www.slideserve.com/moeshe/auditory-scene" target="_blank">Image source</a></i></h5>
                    <p>I designed an experiment which aimed to answer two questions:</p>
                    <ul>
                        <li><p>Are people with more musical experience more adept at auditory scene analysis (ASA) in the case of recorded music than those with less experience?</p></li>
                        <li><p>Does the presence of vocals have a greater effect on performance in this task for those with less musical experience?</p></li>
                    </ul>
                    <br>
                    <p>I had 20 subjects participate in the experiment and used R to conduct data analysis with the results. I found that musical experience is positively correlated with performance on ASA tasks, and the presence of vocals leads to worsened performance across levels of experience.</p>
                    <p><a href="https://github.com/robkellems/Auditory-Scene-Analysis-And-Musical-Experience" target="_blank">Github</a></p>
                    <p><a href="resources/Q370FinalPaper.pdf" target="_blank">Paper</a></p>
                </div>
            </li>
        </ul>
        <h2>2020:</h2>
        <ul>
            <li>
                <h3 onclick="toggleContent('content9')"><b>Bed and Breakfast Database</b></h3>
                <div id="content9" class="content">
                </div>
            </li>
            <li>
                <h3 onclick="toggleContent('content10')"><b>Melody Generation w/ Genetic Algorithms</b></h3>
                <div id="content10" class="content">
                </div>
            </li>
        </ul>
    </div>
</div>
</body>
</html>
